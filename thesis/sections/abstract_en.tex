Sentence embeddings have become ubiquitous in the field of Natural Language Processing (NLP). Despite the incredible variety of sentence embedding algorithms, it is still not entirely clear what aspects of sentences are captured by such representations. However, an enormous amount of research has already been conducted on the evaluation of sentence embeddings in the English language. Unfortunately, languages different from English are considered less frequently; and if they are taken into account, the multi-lingual setting is often restricted to higher-resource languages like German, French or Spanish. Languages for which significantly less resources are available are on the other hand investigated only rarely. This circumstance motivates to put such low-resource languages more in focus and to analyze whether the results reported for English are in general transferable to these languages as well or whether the results turn out to be entirely different. This work adopts the methodology of \textit{probing tasks} popularized by \citep{Conneau.2018a} and extends this concept to a multi-lingual setting including the following five languages: English, German, Russian, Turkish and Georgian. In this work, seven probing tasks are implemented for Georgian and nine for the remaining four languages. Additionally, the embeddings are evaluated on a set of three \textit{downstream applications}. The results in these two task types are subsequently correlated in order to provide further insight into how predictive probing tasks are for downstream task performance. The correlations turn out to be language-dependent. Results for low-resource languages differ substantially from the ones obtained for English.

The fact that the evaluation setting in the context of this work partially produced results which did not comply with the literature motivated an additional stability analysis. The outcome of this experiment suggests that the usage of large and balanced data sets produces results which are more comparable with the literature. The evaluation on data sets comprising at least 30k instances is recommended since the ordering of the embeddings becomes more stable as a consequence of more training data. The effect of hyper-parameter tuning on the other hand was found to be less significant, but usually has positive effects. The evaluation in the context of this thesis is mostly done on data sets comprising 10k instances which are partially imbalanced. Therefore, the results have to be treated with caution. It is advisable that future work in this domain incorporates the recommendations given above.