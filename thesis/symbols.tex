\begin{tabbing}
	\hspace*{2mm}\=\hspace*{2.15cm}\=\kill
	\> $\mathcal{B}$				\>	data batch									\\[3mm]
	\> $c$						\>	index of center word into cocabulary $\mathcal{V}$	\\[3mm]
	\> $d$ 						\>	number of (embedding) dimensions				\\[3mm]
	\> $\mathcal{D}$				\>	data set									\\[3mm]
	\> $\mathfrak{D}$				\>	downstream task							\\[3mm]
	\> $\bm{E}$					\> 	word embedding matrix						\\[3mm]
	\> $\mathbb{E}$				\>	embedding space							\\[3mm]
	\> $\bm{h}$					\>	encoder output								\\[3mm]
	\> $\mathcal{J}(\bm{\theta})$		\>	loss function								\\[3mm]
	\> $m$						\>	window size								\\[3mm]
	\> $o$						\>	index of context word into vocabulary $\mathcal{V}$	\\[3mm]
	\> $\mathfrak{P}$				\>	probing task								\\[3mm]
	\> $\mathbb{R}$				\>	set of real numbers							\\[3mm]
	\> $r_p$						\>	Pearson correlation coefficient					\\[3mm]
	\> $r_s$						\>	Spearman correlation coefficient					\\[3mm]
	\> $rank(\bullet)$				\>	rank of a number $\bullet$ in a list of numbers		\\[3mm]
	\> $s_i$ 						\> 	$i$-th sentence of corpus						\\[3mm]
	\> $\bm{S}$					\>	embedding matrix of a sentence					\\[3mm]
	\> $T$						\>	length of a sentence / number of words in the corpus	\\[3mm]
	\> $\bm{v}_s$					\>	sentence embedding							\\[3mm]
	\> $\bm{v}_w$					\>	word embedding							\\[3mm]
	\> $\widehat{\bm{v}}$			\>	one-hot vector								\\[3mm]
	\> $\mathcal{V}$				\>	set of (unique) vocabulary words					\\[3mm]
	\> $\delta_{ant}$				\>	hyper-parameter of \textit{Attract-Repel} (antonymy)	\\[3mm]
	\> $\delta_{syn}$				\>	hyper-parameter of \textit{Attract-Repel} (synonymy)	\\[3mm]
	\> $\bm{\theta}$				\> 	learnable parameters	of a machine learning model	\\[3mm]
	\> $\tau$						\>	hinge loss									\\[3mm]
	\> $\bullet^{\intercal}$			\>	transpose of a vector or a matrix 				\\[3mm]
	\> $\oplus$ / [$\bullet, \bullet$]		\>	concatenation operator						\\[3mm]
	\> $\odot$						\>	Hadamard/point-wise product					\\[3mm]
	\> $\Vert \bullet \Vert$			\>	norm of a vector								\\	
\end{tabbing}